# 03. 카프카 디자인

배치 전송, 파티션, 분산 기능, 리플리케이션, 자동 파티션 리더 선출 등

## 1. 카프카 디자인의 특징

높은 처리량, 빠른 메세지 전송에 중점

- 분산 시스템
    - 장점
        - 높은 성능
        - 일부 노드 장애 발생하면 다른 노드가 대신 처리
        - 시스템 확장 용이
    - 링크드인은 60대 브로커 까지 사용하고 있음
- 페이지 캐시
    - 처리량을 높히기 위하여 페이지 캐시 이용
    - 잔여 메모리를 이용해 디스크에서 읽고 쓰지 않고 페이지 캐시를 통해 읽고 씀
    - JVM 힙 사이즈는 5G면 충분. 남아있는 메모리는 페이지 캐시로 사용 권장
- 배치 전송 처리
    - 작은 I/O를 묶어 처리해 네트워크 오버헤드 감소

## 2. 카프카 데이터 모델

- 토픽
    - 메세지를 받을 수 있도록 논리적으로 묶은 개념
- 파티션
    - 토픽을 구성하는 데이터 저장소로 `수평 확장이 가능한 단위`
    - 빠른 전송과 처리량을 위해서는 토픽의 파티션과 그 수 만큼 프로듀서 수도 늘려줘야 함
- 오프셋
    - 파티션 마다 메세지가 저장되는 위치
    - 오프셋은 파티션 내(에서만) 유일, 순차적으로 증가하는 숫자 형태
    - 오프셋을 이용해 `메세지의 순서를 보장` (오프셋 순서가 바뀐 상태로 컨슘 불가능)

### 무조건 파티션 수를 늘려야 하나?

- 적절한 값 설정이 필요. 많은 파티션 수의 단점
    - 파일 핸들러 낭비
        - 각 파티션은 브로커의 디렉토리와 매핑. 저장되는 데이터마다 2개의 파일(인덱스, 실제 데이터)가 생성됨
        - 카프카는 모든 디렉토리 파일에 대해 파일 핸들을 열게 되므로, 파티션의 수가 많을 수록 파일 핸들 수 역시 많아짐 (리소스 낭비)
    - 장애 복구 시간 증가
        - 각 브로커에는 여러 개의 파티션이 존재하고 각 파티션마다 `리플리케이션`이 동작
            - 브로커가 다운되면 해당 브로커에 리더가 있는 파티션은 일시적으로 사용할 수 없게 되므로, 팔로워 중 하나를 리더로 선출 (by 컨트롤러 브로커)
            - 파티션 수가 많으면 컨트롤러가 파티션 별 새 리더를 선출하는데 시간 많이 소요됨 (일부 파티션 장애 시간이 길어짐)
        - 다운된 브로커가 컨트롤러라면, 다른 브로커로 컨트롤러가 넘어갈 때 까지 새로운 리더를 선출할 수 없음
            - 새 컨트롤러를 초기화 하는 동안 주키퍼에서 모든 파티션 데이터를 읽어야 함 (?)

### 내 토픽의 적절한 파티션 수는?

- 목표 처리량의 기준에 따라 설정 필요
- `프로듀서의 초당 메세지 수`에 따라, 카프카에서 받아서 처리할 수 있기 위한 파티션 개수 설정
- `컨슈머의 초당 메세지 처리량`에 따라, 파티션 수는 컨슈머 수와 동일하게 맞춰야 (컨슈머마다 각각의 파티션에 접근할 수 있도록)
- 팁
    - 파티션 개수는 늘릴 수만 있고 줄일 수는 없음
    - 파티션 수를 줄이고 싶다면, 토픽을 삭제하고 다시 만들어야 함
    - 일단 적은 수의 파티션으로 운영해보고, 프로듀서 또는 컨슈머의 `병목이 발생하면 파티션 수와 프로듀서, 컨슈머 수를 늘리는 것이 좋음`
    - 카프카에서는 브로커 당 약 2,000 개 정도의 최대 파티션 개수를 권장

## 3. 카프카의 고가용성과 리플리케이션

고가용성 보장을 위해 `토픽을 이루는 각각의 파티션`을 리플리케이션 (토픽 자체 리플리케이션 X)

- Replication Factor
    - `default.replication.factor` 설정 (default 1)
    - 토픽 별 리플리케이션 팩터 값 설정 가능 (설정 없으면 default 설정을 따름)
- 단점
    - 중복 데이터로 저장소 크기가 많이 필요
    - 완벽한 리플리케이션 보장을 위해 비활성화된 토픽이 리플리케이션을 잘 하고 있는지 토픽의 상태를 체크하는 작업 (?) → 브로커의 리소스 사용량 증가
    - 결론: 모든 토픽에 Replication factor 3을 적용하기 보다, 데이터 중요도에 따라 설정 필요
- 리더와 팔로워
    - `모든 읽기와 쓰기는 리더를 통해서만`
    - 팔로워는 리더의 데이터를 복제만 하고 읽기, 쓰기에 관여하지 않음
    - 리더와 팔로워는 동일한 오프셋, 메세지를 가짐
    - 리더의 브로커에서 장애가 발생하면, 팔로워의 브로커가 해당 파티션의 리더가 됨
    - 팔로워에 문제가 있어 리더로 부터 데이터를 가져오지 못하면 데이터 정합 문제 발생 → ISR
- ISR (In Sync Replica)
    - 현재 리플리케이션 되고 있는 리플리케이션 그룹
    - `ISR에 속해있는 구성원만 리더의 자격`을 가질 수 있음
    - 리더는 팔로워들이 주기적으로 데이터를 잘 가져가는지 확인
    - 일정 주기(`replica.lag.time.max.ms`) 만큼 확인 요청이 오지 않으면, 해당 팔로워를 ISR 그룹에서 추방 (리더 자격 박탈)

## 4. 모든 브로커가 다운된다면

- 모든 브로커가 다운됐을 때 선택할 수 있는 방법 두 가지
    - 마지막 리더가 살아나기를 기다린다 (`unclean.leader.election.enable = false`, default)
        - `일관성` 우선
        - `메세지 손실 없음`
        - 필수 조건: 재시작시 마지막 리더가 반드시 시작되어야 함 (마지막 리더가 살아나지 않으면 `장애 장기화`)
    - ISR 에서 추방되었지만 먼저 살아나면 자동으로 리더가 된다 (`unclean.leader.election.enable = true`)
        - `가용성` 우선
        - ISR 에서 추방된 팔로워가 리더가 되면 `메세지 손실 발생`
        - 브로커 하나라도 빠르게 정상화되면 장애 정상화 가능

## 5. 카프카에서 사용하는 주키퍼 znode의 역할

- `/controller`
    - 현재 카프카 클러스터의 컨트롤러 정보 저장
    - 클러스터 내 `브로커 하나를 컨트롤러로 선정`해, 컨트롤러는 브로커의 실패를 감지하고 `실패한 브로커에 의해 영향받는 모든 파티션의 리더를 변경` 책임
    - 컨트롤러가 다운되면 클러스터 내 남은 브로커가 새로운 컨트롤러가 됨
- `/brokers`
    - 브로커 관련 정보(`broker.id`) 저장
    - 브로커는 시작 시 `brokers/ids` 에 `broker.id` 값을 임시 노드(ephemeral node)를 사용해 저장 (브로커가 종료되면 해당 znode는 사라짐)
    - `brokers/topics` znode에서는 클러스터 내 토픽 정보(파티션 수, ISR 구성 정보, 리더 정보 등) 확인 가능
- `/consumers`
    - 컨슈머 관련 정보 저장
    - 오프셋(컨슈머가 각각의 파티션의 어디까지 읽었는지) 값을 영구 노드(persistent node)를 사용해 저장
    - 주키퍼에 오프셋을 저장하는 방법은 곧 지원 종료
- `/config`
    - 토픽의 상세 설정 정보
- 참고
    - 영구 노드: delete 호출하여 삭제 가능 (ex. 오프셋 정보 저장)
    - 임시 노드: 생성한 클라이언트와 연결이 끊어지거나 장애가 발생하면 삭제 (ex. 브로커 id)
