# 1. 리플리케이션

여러 대의 브로커를 구성했음에도 한두 대에서 장애가 발생하더라도 안전하게 서비스를 운영할 수 있도록 하기 위해 리플리케이션이라는 개념이 등장했다.

리플레리케이션을 사용하려면 토픽 생성 시 필수값으로 `replication factor`라는 옵션을 설정해야한다. 

이 리플리케이션으로 데이터 유실을 방지하고 유연성을 확보할 수 있게 해준다.

## 1.1 동작 개요

```shell
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3
```

위와 같은 명령어로 토픽을 생성하면서 리플리케이션 옵션을 지정할 수 있다.

이렇게 토픽을 생성하면 클러스터 내의 브로커들은 해당 토픽에 대한 레코드를 동일하게 갖고 있을 수 있다. 이렇게 되는 이유가 리플리케이션이 되어있기 때문이다.

따라서 총 3개의 리플리케이션이 있으므로 2대의 브로커가 다운되어도 남은 1대의 브로커가 메세지를 처리할 수 있다.

이렇게 리플리케이션을 구성하면 각 브로커들은 `리더`, `팔로워`라는 역할로 구분되는데 `리더는 단 1개의 노드`, `나머지 노드들은 팔로워`가 된다.

---

## 1.2 리더와 팔로워

리더는 리플리케이션들 중 단 하나만 선택되고 모든 메세지는 이 리더에게만 메세지를 보내게 된다. 또한 컨슈머도 리더에게 메세지를 가져오는 형태로 운영된다.

그런데, 리더에게서만 읽기/쓰기 작업을 수행한다해서 팔로워가 하는 역할이 없는 것은 아니다. 리더가 문제가 발생할 경우 그 자리를 대체해야하기 때문에

`팔로워는`리더가 메세지를 받았는지 확인하고 새로운 메세지가 있으면 그 `메세지를 복제하기 위한 풀링 작업`을 계속 하고 있다.

---

## 1.3 복제 유지와 커밋

### 1.3.1 ISR

리더와 팔로워는 `ISR(InSyncReplica)`이라는 `논리적 그룹`으로 묶여있다. 리더와 팔로우를 하나의 그룹으로 묶어서 해당 그룹의 리더가 문제가 발생하면 해당 그룹의 팔로워가 그 자리를 대체하기 위함이다.

즉, `ISR그룹에 속하지 않은 팔로워들은 현재 불용상태가 된 리더를 대체할 수 없다.`

ISR내의 팔로워들은 리더와의 데이터 정합성을 위해 리더를 풀링하고 있고, 리더는 팔로워들이 메세지를 전달 받을 때까지 기다린다. 하지만 예기치못한 에러들로 리플리케이션을 수행하지 못한 팔로워가 있을 때 계속 풀링에 성공하게끔 하면 안된다.

추후 해당 팔로워가 리더로 승격되었을 때 데이터 유실의 문제가 발생할 수 있기 때문이다. 또한 리플레케이션이 잘 동작하지 않은 팔로워는 리더로 승격할 수 없다.

따라서 `리더는 팔로워가 메세지를 잘 전달받고 있는지 감시`하는 역할도 한다.

### 1.3.2 리플리케이션을 잘 하고 있나?

리더는 팔로워가 리플리케이션을 잘 수행하는지 판단한다. 만약 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않는다면 해당 팔로워를 ISR에서 추방한다. 위에서 언급했듯이 리플리케이션을 제대로 수행하지 못한 팔로워는 리더가 될 수 없는 이유가 된다.

ISR내에서 모든 팔로워가 복제를 마치면 리더는 내부적으로 커밋되었다는 표시를 하고, 마지막 커밋 오프셋 위치는 `High WaterMark`라고 한다.

### 1.3.3 시나리오로 알아보는 커밋의 필요성

0. `replication-factor`를 `3`으로 지정하여 하나의 토픽을 생성했다.
1. `리더` 및 `팔로워`가 `"First Message"`라는 메세지를 모두 받았다. 따라서 커밋이 완료되었다.
2. `리더`가 `"Second Message"`라는 메세지를 받았지만 `팔로워는 모두 해당 메세지를 복제하지 못한` 상태이다. 따라서 아직 커밋되지 않았다.

이 상황에서 `2개의 컨슈머`가 메세지를 컨슘하는데, 이 때 마침 `리더가 기존 팔로워로 변경`된다면 어떻게 될까?

`A`라는 컨슈머는 `리더가 변경되기 직전`에 메세지를 읽어 `"Second Message"`를 읽었다.

`B`라는 컨슈머는 `리더가 변경된 후` 메세지를 읽어 기존 복제되지 않은 팔로워였던 현재 리더에 의해 `"First Message"`라는 메세지를 읽게 됐다.

결과적으로 메세지 불일치 현상이 발생했고 카프나는 이런 현상을 방지하고자 커밋이 완료된 메세지만 읽도록 구현되어있다.

### 1.3.4 커밋된 위치를 어떻게 알 수 있을까?

모든 브로커는 시작될 때 커밋된 메세지를 유지하기 위해 `replication-offset-checkpoint`라는 파일에 마지막 커밋 오프셋을 저장한다.

```shell
cat /data/kafka-logs/replication-offset-checkpoint
```
위 명령어로 직접 확인할 수 있다.


**`정리하자면 카프카는 데이터 무결성을 위해 내부적으로 복제 유지를 위해 커밋된 오프셋의 마지막 위치를 로그로 기록하고 있다.`**

---

## 1.4 리더와 팔로워의 단계별 리플리케이션 동작

- 리더에 메세지가 쓰기된다.
- 팔로워들은 리더에게 `오프셋 0`에 대한 메세지를 `Fetch`요청하여 새로운 메세지가 있다면 리플리케이션한다.
  - 이 때 리더는 팔로워들로 부터 메세지를 리플리케이션했다는 `ACK`신호를 받지 않는다.
- 리플리케이션을 마친 팔로워가 `오프셋 1`을 `fetch`요청한다.
  - 만약 팔로워가 리플리케이션에 실패했다면 오프셋 0을 요청한다.
- 리더는 오프셋 0에 대해 커밋 표시를 한 후 하이워터마크를 증가시킨다.

이렇게 리더는 팔로워가 fetch요청하는 오프셋으로 현재 팔로워들이 따라오고 있는 상황을 파악할 수 있게 된다.

다른 메세징 시스템에는 ACK요청을 받으면서 리더/팔로워 간의 데이터 정합성을 맞추지만 카프카는 그 과정을 제거하여 성능 향상을 이끌어냈다.

---

## 1.5 리더에포크(LeaderEpoch)

파티션들이 복구 동작을 할 때 메세지의 일관성을 유지하기 위한 용도로 컨트롤러로부터 관리되는 32bit 숫자로 표현된다.

이 리더에포크 정보는 리플리케이션 프로토콜로 전파되고 새로운 리더가 변경된 후 변경된 리더에 대한 정보는 팔로워에게 전달된다.

리더에포크는 복구 동작 시에 하이워터마크를 대체하기도 한다.

### 1.5.1 시나리오로 알아보는 리더에포크의 필요성

1. `리더`가 `"First Message"`라는 메세지를 받은 후 0번 오프셋에 저장했다.
2. `팔로워`는 `0번 오프셋`을 fetch하여 리플리케이션을 완료했다.
3. `리더`가 `"Second Message"`라는 메세지를 받은 후 `1번 오프셋`에 저장했다.
4. `팔로워는` `오프셋 1`에 대해 `fetch요청`을 한 후 응답으로 받은 `하이워터마크를 자신에게 반영한다.`
5. `팔로워가` `오프셋 1`에 대한 메세지를 리플리케이션한다.
6. `팔로워는` `오프셋 2`에 대해 `fetch요청`을 보낸다.
7. `리더`는 해당 요청을 받은 후 `하이워터마크를 2로` 올렸다.
7. `팔로워`는 `오프셋 1`인 `"Second Message"`까지 리플리케이션을 완료했지만 리더로부터 `하이워터마크를 2로 올리는 내용은 받지 못했다.`
8. 예기치 못한 에러로 `팔로워가 다운됐다.`

현재 상황

| 리더                    | 팔로워                  |
|-----------------------|----------------------|
| "FirstMessage" 오프셋 0  | "FirstMessage" 오프셋 0 |
| "SecondMessage" 오프셋 1 |                      |
| 하이워터마크 == 2           | 하이워터마크 == 1          |

카프카 내부 복구 동작에 의한 다시 시나리오를 진행한다.

- 팔로워는 자신의 워터마크보다 높은 메세지들은 삭제한다.
  - 따라서 오프셋 1의 "Second Message"는 삭제된다.
- 팔로워는 리더에게 오프셋 1의 메세지를 fetch 요청한다.
- 이 순간 리더가 불용상태로 들어가게되어, 해당 파티션에 유일하게 남아있는 팔로워가 새로운 리더로 승격한다.

현재 상황

| 뉴리더                   |
|-----------------------|
| "FirstMessage" 오프셋 0  |
| 하이워터마크 == 1           |

결국 새로운 리더가 선출되었지만 메세지 유실이 발생한 상황이 되었다.

이 때 사용할 수 있는게 리더에포크이다. 리더에포크를 사용하면 복구 동작 중 팔로워의 하이워터마크보다 높은 메세지를 즉시 삭제하는 것이 아니라 리더에게 리더에포크 요청을 보낸다.

리더에포크를 사용한 복구 과정은 아래와 같다.

1. 리더에게 `리더에포크 요청`을 보낸다.
2. 리더는 `팔로워에게` `리더에포크 응답`으로 `"오프셋 1의 "Second Message"까지"` 라고 보낸다.
3. 팔로워는 자신의 하이워터마크보다 높은 오프셋 1의 "Second Message"를 `삭제하지 않고 리더의 응답을 확인`한다.
4. `"Second Message"까지 자신의 하이워터마크를 상향`한다.

---

# 2. 컨트롤러

카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 수행한다. 컨트롤러는 브로커가 실패하는 것을 모니터링하는 역할로, 파티션의 ISR 리스트 중에서 리더를 선출한다.

## 2.1 예기치 못한 장애 발생 시 리더 선출 과정

1. 주키퍼가 브로커와 연결이 끊어짐을 감지한다. (하트비트 신호를 받지 못했을 때 이다.)
2. 파티션의 ISR에 변화가 생겼음을 감지하여 해당 ISR중 하나를 리더로 선출한다. (선출 과정에는 투표를 통해 쿼럼을 만족시키는 것을 택한다.)
3. 컨트롤러는 0번 파티션의 새로운 리더에 대한 정보를 주키퍼에 기록한다.
4. 클러스터 내의 변경사항은 모든 브로커들에게 전파된다.

## 2.2 일부러 종료시켰을 때 리더 선출 과정

1. 브로커 종료 명령어를 실행하여 `SIG_TERM`신호를 브로커에 전달한다.
2. `SIG_TERM`신호를 받은 브로커는 컨트롤러에게 알린다.
3. 컨트롤러가 리더 선출 작업을 진행하고 주키퍼에 기록한다.
4. 컨트롤러가 새로운 리더 정보를 브로커들에게 전송한다.
5. 컨트롤러는 정상 종료를 허가한다는 응답을 종료 요청을 보낸 브로커에 보낸다.
6. 응답을 받은 브로커는 캐시에 있는 내용을 디스크에 저장한 후 종료한다.

위 두 가지 사례의 가장 큰 차이점은 `다운타임`이다. 제어된 종료를 하면 이 시간을 최소화할 수 있는데, 브로커가 종료되기 전 해당 브로커가 리더로써 할당된 전체 파티션에 대해 리더 선출 작업을 진행하기 때문이다.

제어된 종료를 사용하기 위해선 `controlled.shutdown.enable=true`옵션이 적용되어야한다.

---

# 3. 로그 (로그 세그먼트)

토픽으로 들어오는 메세지는 세그먼트라는 파일에 저장된다. 세그먼트는 `메세지의 내용`뿐만 아니라 `키`, `밸류`, `오프셋`, `메세지 크기` 등 메타데이터를 저장하고 이 내용들을 브로커의 `로컬 디스크에 보관`한다.

로그 세그먼트의 최대 크기는 1GB로 기본값 설정되어있다. 만약 이 용량보다 커질 경우에는 rolling 전략을 사용하여 임계치에 다다랐을때 새로운 로그 세그먼트를 생성하는 방식으로 진행한다.

## 3.1 로그 세그먼트 네이밍 규칙

로그 세그먼트 파일명은 카프카가 자동으로 지어주는데 이 때 규칙이 있다. 오프셋의 시작 번호를 이용해 파일 이름을 생성하는 규칙을 따른다.

즉, 오프셋 1에 대한 로그 세그먼트라면 `001.log`라는 이름을 가진 로그 세그먼트가 생성될 수 있다.

## 3.2 로그 세그먼트 관리 방법 2가지

### 3.2.1 로그 세그먼트 삭제

말 그대로 로그 세그먼트를 삭제한다는 것이다. 이 때 로그 세그먼트 보관 시간이 특정 숫자보다 크면 삭제하는 옵션을 사용할 수 있는데 `retention.ms`라는 옵션이다. 이 옵션의 값보다 보관한 기간이 더 크면 해당 로그 세그먼트는 삭제된다.

또한 삭제 작업은 일정한 주기를 가지고 체크하는데 기본값은 5분으로 맞춰져있다.

### 3.2.2 로그 세그먼트 컴팩션

로그를 삭제하지 않고 압축하여 보관하는 방식이다. 로컬 디스크에 저장되어있는 세그먼트들을 대상으로 하여 현재 활성화 되어있는 세그먼트를 제외하고 컴팩션을 진행한다.

하지만 컴팩션을 해도 보관기간이 길어진다면 용량은 감당할 수 없을 것이기 때문에 카프카에서는 메세지를 컴팩션하는 것이 아니라 메세지의 키값을 기준으로 마지막의 데이터를 보관한다.

컨슈머의 그룹 정보를 저장하는 토픽인 `__consumer_offset`이 실제로 로그 세그먼트 컴팩션을 통해 관리되어지고 있다. 