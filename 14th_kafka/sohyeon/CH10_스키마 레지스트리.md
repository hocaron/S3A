# 10장 스키마 레지스트리
## 10.1 스키마의 개념과 유용성
- **카프카에 스키마가 없다면,** 누군가의 실수로 정의되지 않은 형태의 데이터를 해당 토픽으로 보낸다면, 그와 연결된 모든 시스템이 영향을 받게 될 것이고, 심각한 문제를 야기할 수 있다.
- 데이터를 컨슘하는 여러 부서에게 그 데이터에 대한 정확한 정의와 의미를 알려주는 역할을 하는 것이 **스키마**이다.
- 카프카에서 스키마의 진화를 지원하여, 카프카를 사용하는 수십~수백의 애플리케이션들이 별다른 영향 없이 스키마를 변경할 수 있다.
- 👎 데이터를 처리하기에 앞서 스키마를 정의하는 작업에는 많은 시간과 노력이 든다. (하지만 이를 감수할만큼 이점이 많음)

<br/>

## 10.2 카프카와 스키마 레지스트리
### 10.2.1 스키마 레지스트리 개요

<img alt="image" width=500 src="https://github.com/mash-up-kr/S3A/assets/55437339/912c034f-d2a7-4228-b7a6-2c216fe47d1d"/>

🔼 스키마 레지스트리 구성도
- 스키마 레지스트리는 프로듀서/컨슈머와 직접 통신한다.
- 프로듀서는 스키마 레지스트리에 스키마를 등록하고, 스키마 레지스트리는 등록된 스키마 정보를 카프카의 내부 토픽에 저장한다.
- 프로듀서는 등록된 스키마의 ID와 메시지를 카프카로 전송하고, 컨슈머는 프로듀서가 전송한 스키마 ID와 메시지를 조합해 읽을 수 있다.
- 스키마 레지스트리를 이용하기 위해서 지원되는 데이터 포맷을 사용해야 하는데, `에이브로`가 가장 대표적이다.

<br/>

### 10.2.2 스키마 레지스트리의 에이브로 지원
- `에이브로`: 시스템, 프로그래밍 언어, 프로세싱 프레임워크 사이에서 데이터 교환을 도와주는 오픈소스 직렬화 시스템
  - 빠른 바이너리 데이터 포맷을 지원한다.
  - JSON 형태의 스키마를 정의할 수 있는 매우 간결한 데이터 포맷이다.
 
- 스키마 레지스트리는 에이브로 포맷을 가장 먼저 지원했고, 최근에는 JSON, 프로토콜 버퍼 포맷도 지원한다.

<br/>

> **스키마 예제 파일 생성**

```json
{
  "namespace": "student.avro", # 이름을 식별하는 문자열
  "type": "record", # 에이브로는 record, enums, arrays, maps 등을 지원
  "doc": "Ths is an example of Avro", # 사용자들에게 이 스키마 정의에 대한 설명 제공
  "name": "Student", # 레코드의 이름을 나타내는 문자열 (필숫값)
  "fields": [ # JSON 배열로서, 필드들의 리스트를 뜻함
    # name: 필드 이름, type: 데이터 타입 정의, doc: 주석
    {"name": "name", "type": "string", "doc": "Name of the student"},
    {"name": "class", "type": "int", "doc": "Class of the student"}
  ]
}
```
🔼 에이브로를 활용한 학생 명단에 대한 스키마 정의 파일
- 데이터 필드마다 데이터 타입을 정의할 수 있다.
- doc를 이용해 각 필드의 의미를 사용자들에게 정확하게 전달할 수 있다.

<br/>

### 10.2.3 스키마 레지스트리 설치
```
$ cd kafka2/
$ cd chapter2/ansible_playbook
$ ansible-playbook -i hosts kafka.yml
```
🔼 카프카 클러스터 설치

<br/>

```
$ cd kakfa2/chapter2/ansible_playbook
$ ansible-playbook -i hosts site.yml
```
🔼 주키퍼, 카프카 재설치

<br/>

```
$ sudo wget http://packages.confluent.io/archive/6.1/confluent-community-6.1.0.tar.gz -0 /opt/confluent-community-6.1.0.tar.gz
$ sudo tar zxf /opt/confluent-community-6.1.0.tar.gz -C /usr/local
$ sudo ln -s /usr/local/confluent-6.1.0 /usr/local/confluent
```
🔼 파일 다운로드, 압축 해제, 심볼릭 링크 설정

<br/>

```
vi /usr/local/confluent/etc/schema-registry/schema-registry.properties
```
🔼 파일 열기

<br/>

```
listeners=http://0.0.0.0:8081 # 스키마 레지스트리에서 사용할 TCP 포트를 8081 포트로 지정
kafkastore.bootstrap.servers=PLAINTEXT://peter-kafka01.foo.bar:9092,peter-kafka02.foo.bar:9092,peter-kafka03.foo.bar:9092 # 스키마의 버전 히스토리 및 관련 데이터를 저장할 카프카 주소를 입력
kafkastore.topic=_schemas # 스키마의 버전 히스토리 및 관련 데이터 저장 토픽의 이름을 _schemas로 지정
schema.compatibility.level=full # 스키마 호환성 레벨을 full로 설정
```
🔼 스키마 레지스트리 옵션 설정

<br/>

```
$ sudo vi /etc/systemd/system/schema-registry.service
```
🔼 코드부를 입력하기 위한 파일 열기

<br/>

```
[Unit]
Description=schema registry
After=network.target

[Service]
Type=simple
ExecStart=/usr/local/confluent/bin/schema-registry-start /usr/local/confluent/etc/schema-registry/schema-registry.properties
Restart=always

[Install]
WantedBy=multi-user.target
```
🔼 스키마 레지스트리의 system 설정

<br/>

```
$ sudo systemctl daemon-reload
$ sudo systemctl start schema-registry
```
🔼 스키마 레지스트리 실행

<br/>

|옵션|설명|
|---|---|
|GET /schemas|현재 스키마 레지스트리에 등록된 전체 스키마 리스트 조회|
|GET /schemas/ids/id|스키마 ID로 조회|
|GET /schemas/ids/id/versions|스키마 ID의 버전|
|GET /subjects|스키마 레지스트리에 등록된 subject 리스트<br/>subject는 토픽이름-key, 토픽이름-value 형태로 쓰임|
|GET /subjects/서브젝트 이름/versions|특정 서브젝트의 버전 리스트 조회|
|GET /config|전역으로 설정된 호환성 레벨 조회|
|GET /config/서브젝트 이름|서브젝트에 설정된 호환성 조회|
|DELETE /subjects/서브젝트 이름|특정 서브젝트 전체 삭제|
|DELETE /subjects/서브젝트 이름/versions/버전|특정 서브젝트에서 특정 버전만 삭제|

🔼 스키마 레지스트리 API

<br/>

## 10.3 스키마 레지스트리 실습
### 10.3.1 스키마 레지스트리와 클라이언트 동작

<img alt="image" width="600" src="https://github.com/mash-up-kr/S3A/assets/55437339/6c48e4a4-5564-4f69-a4f9-3a7c44ba1463"/>

🔼 스키마 레지스트리와 클라이언트 동작
1. 에이브로 프로듀서는 스키마 레지스트리의 스키마가 유효한지 여부를 확인한다. (확인되지 않으면, 스키마 등록/캐시)
2. 스키마 레지스트리는 현 스키마가 저장소에 저장된 스키마와 동일한지, 진화한 것인지 확인한다. (문제 없으면 프로듀서에게 고유 ID 응답)
3. 프로듀서는 받은 스키마 ID를 참고하여 메시지를 카프카로 전송한다.
4. 에이브로 컨슈머는 스키마 ID로 토픽에 저장된 메시지를 읽는다. (컨슈머에게 스키마 ID가 없다면 스키마 레지스트리에서 가져옴)

<br/>

### 10.3.2 파이썬을 이용한 스키마 레지스트리 활용
```
$ sudo yum -y install python3
$ python3 -m venv venv10
$ source venv10/bin/activate
$ pip install confluent-kafka[avro]
```
🔼 파이썬 설치, 가상 환경(venv10) 생성, 라이브러리 설치

<br/>

```python
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro",
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
"""
# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)
value = {"name": "Peter", "class": 1} # 전송할 메시지


# 전송 결과 확인
def delivery_report(err, msg):
  """ Called once for each message produced to indicate delivery result.
      Triggered by poll() or flush(). """
  if err is not None:
    print('Message delivery failed: {}'.format(err))
  else:
    print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition())

# AvroProducer 속성 정의
avroProducer = AvroProducer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'on_delivery': delivery_report,
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'
  }, default_value_schema=value_schema)

# 메시지 전송
avroProducer.produce(topic='peter-avro2', value=value)
avroProducer.flush()
```
🔼 에이브로 메시지 전송을 위한 기본 파이썬 에이브로 프로듀서(python-avro-producer.py)

<br/>

```
$ python python-avro-producer.py

# 출력
Message delivered to peter-avro2 [0]
```
🔼 프로듀서 실행

<br/>

```python
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer
from confluent_kafka.avro.serializer import SerializerError

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro",
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
"""
# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)

# AvroConsumer 속성 정의
c = AvroConsumer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'group.id': 'python-groupid01',
  'auto.offset.reset': 'earliest',
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'}, reader_value_schema=value_schema)

# 토픽 구독
c.subscribe(['peter-avro2'])

# 메시지 컨슘
while True:
  try:
    msg = c.poll(10)

  except SerializerError as e:
    print("Message deserialization failed for {}: {}".format(msg, e))
    break

  if msg is None:
    continue

  if msg.error():
    print("AvroConsumer error: {}".format(msg.error()))
    continue
    print(msg.value())

# 종료
c.close()
```
🔼 에이브로 메시지를 가져오기 위한 기본 파이썬 에이브로 컨슈머(python-avro-consumer.py)
- peter-avro2 토픽을 컨슘한다.
- 컨슈머 그룹 ID는 python-groupid01로 처음 오프셋부터 메시지를 읽어온다.

<br/>

```
$ python python-avro-consumer.py

# 출력
{'name': 'Peter', 'class': 1}
```
🔼 컨슈머 실행

<br/>

```
$ curl http://peter-kafka03.foo.bar:8081/schemas | python -m json.tool
```
- 스키마 레지스트리의 API를 이용해 스키마가 스키마 레지스트리에 잘 기록되었는지 확인할 수 있다.

<br/>

```python
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro",
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "first_name", "type": ["null", "string"], "default": null, "doc": "First name of the student"},
    {"name": "last_name", "type": ["null", "string"], "default": null, "doc": "Last name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
"""
# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)
value = {"first_name": "Peter", "last_name": "Parker", "class": 1} # 전송할 메시지

# 전송 결과 확인
def delivery_report(err, msg):
  """ Called once for each message produced to indicate delivery result.
      Triggered by poll() or flush(). """
  if err is not None:
    print('Message delivery failed: {}'.format(err))
  else:
    print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition())

# AvroProducer 속성 정의
avroProducer = AvroProducer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'on_delivery': delivery_report,
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'
  }, default_value_schema=value_schema)

# 메시지 전송
avroProducer.produce(topic='peter-avro2', value=value)
avroProducer.flush()
```
🔼 스키마를 재정의한 파이썬 에이브로 프로듀서(python-avro.producer2.py)
- 스키마의 name 필드를 삭제하고, first_name과 last_name이라는 2개의 필드를 추가한다.
- 스키마 레지스트리를 사용하고 있으므로 비교적 간단하게 변경된 내용을 적용할 수 있다.

<br/>

```pyhon
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer
from confluent_kafka.avro.serializer import SerializerError

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro",
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "first_name", "type": ["null", "string"], "default": null, "doc": "First name of the student"},
    {"name": "last_name", "type": ["null", "string"], "default": null, "doc": "Last name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
"""

# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)

# AvroConsumer 속성 정의
c = AvroConsumer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'group.id': 'python-groupid02', # 새로운 컨슈머 그룹 아이디 적용
  'auto.offset.reset': 'earliest',
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'}, reader_value_schema=value_schema)

# 토픽 구독
c.subscribe(['peter-avro2'])

# 메시지 컨슘
while True:
  try:
    msg = c.poll(10)

  except SerializerError as e:
    print("Message deserialization failed for {}: {}".format(msg, e))
    break

  if msg is None:
    continue

  if msg.error():
    print("AvroConsumer error: {}".format(msg.error()))
    continue
    print(msg.value())

# 종료
c.close()
```
🔼 스키마를 재정의한 파이썬 에이브로 컨슈머(python-avro-consumer2.py)

<br/>

```
$ python python-avro-consumer2.py

# 출력
{'class': 1, 'first_name': None, 'last_name': None}
{'first_name': 'Peter', 'last_name': 'Parker', 'class': 1}
```
🔼 컨슈머 실행
- 출력 결과를 통해 총 2개의 메시지를 읽었음을 알 수 있다.
  - 첫번째: 스키마가 진화하기 전 프로듀서가 보낸 메시지
  - 두번째: 스키마 진화 후 프로듀서가 보낸 메시지
 
- 진화된 스키마를 컨슈머에 적용했음에도 컨슈머는 스키마가 다른 첫 번째 메시지도 잘 가져온다.

<br/>

```
$ curl http://peter-kafka03.foo.bar:8081/schemas | python -m json.tool

# 출력
...
  "version": 2,
  ...
  "schema": "{...\"first_name...\"last_name"...
...
```
🔼 스키마 레지스트리 API를 이용한 스키마 확인
- 버전이 2로 변경됐다.
- first_name, last_name과 같이 진화된 스키마 내용이 반영됐다.

<br/>

```
curl http://peter-kafka03.foo.bar:8081/subjects/peter-avro2-value/versions | python -m json.tool

# 출력
[
  1,
  2
]
```
🔼 스키마 버전 리스트 확인
- 2개의 스키마 버전이 존재하는 것을 확인할 수 있다.

<br/>

## 10.4 스키마 레지스트리 호환성
### 10.4.1 BACKWARD 호환성
- 진화된 스키마를 적용한 컨슈머가 **진화 전의 스키마가 적용된 프로듀서가 보낸 메시지**를 읽을 수 있도록 허용한다.

<br/>

<img alt="image" width="500" src="https://github.com/mash-up-kr/S3A/assets/55437339/c5eb3efb-7079-4226-aee2-ac4977ce5f47"/>

🔼 BACKWARD 호환성
- 스키마 레지스트리에 버전별로 버전1, 버전2, 버전3까지의 스키마가 저장되어 있다고 가정한다.
- 컨슈머는 자신과 동일한 버전인 버전3 스키마를 사용하는 프로듀서의 메시지를 처리할 수 있다.

<br/>

|호환성 레벨|지원버전(컨슈머 기준)|변경 허용 항목|스키마 업데이트 순서|
|---|---|---|---|
|BACKWARD|자신과 동일한 버전과 하나 아래의 하위 버전|필드 삭제, 기본 값이 지정된 필드 추가|컨슈머 -> 프로듀서|
|BACKWARD_TRANSITIVE|자신과 동일한 버전을 포함한 모든 하위 버전|필드 삭제, 기본 값이 지정된 필드 추가|컨슈머 -> 프로듀서|

🔼 BACKWARD 호환성

<br/>

### 10.4.2 FORWARD 호환성
- **진화된 스키마가 적용된 프로듀서가 보낸 메시지**를 전환 전의 스키마가 적용된 컨슈머가 읽을 수 있게 한다.

<br/>

<img alt="image" width="500" src="https://github.com/mash-up-kr/S3A/assets/55437339/d485aab1-6897-42be-a83a-cf30cacfb50d"/>

🔼 FORWARD 호환성
- 최신 버전의 스키마(버전3)를 이용해 프로듀서가 메시지를 전송할 때, 동일한 버전(3)의 스키마를 사용하는 프로듀서는 프로듀서가 보낸 메시지 처리가 가능하다.
- 한단계 낮은 버전(2) 스키마를 사용하는 컨슈머도 버전3 스키마를 이용하는 프로듀서가 보내는 메시지를 처리할 수 있다.

<br/>

|호환성 레벨|지원 버전(컨슈머 기준)|변경 허용 항목|스키마 업데이트 순서|
|---|---|---|---|
|FORWARD|자신과 동일한 버전과 하나 위의 상위 버전|필드 추가, 기본 값이 지정된 필드 삭제|프로듀서 -> 컨슈머|
|FORWARD_TRANSITIVE|자신과 동일한 버전을 포함한 모든 상위 버전|필드 추가, 기본 값이 지정된 필드 삭제|프로듀서 -> 컨슈머|

🔼 FORWARD 호환성 요약 정리

<br/>

### 10.4.3 FULL 호환성
- BACKWARD와 FORWARD 호환성 모두를 지원한다.
- 가장 최근 2개의 버전 스키마를 지원한다.

<br/>

|호환성 레벨|지원 버전(컨슈머 기준)|변경 허용 항목|스키마 업데이트 순서|
|---|---|---|---|
|FULL|자신과 동일한 버전과 하나 위 또는 하나 아래 버전|필드 추가, 기본 값이 지정된 필드 삭제|순서 상관없음|
|FULL_TRANSITIVE|자신과 동일한 버전을 포함한 모든 상위 버전과 하위 버전|필드 추가, 기본 값이 지정된 필드 삭제|순서 상관없음|

🔼 FULL 호환성 요약 정리

<br/>

### 10.4.4 스키마 레지스트리 호환성 실습

```
listeners=http://0.0.0.0:8081
kafkastore.bootstrap.servers=PLAINTEXT://peter-kafka01.foo.bar:9092,peter-kafka02.foo.bar:9092,peter-kafka03.foo.bar:9092
kafkastore.topic=_schemas
schema.compatibility.level=backward # 호환성 레벨 설정
```
🔼 스키마 레지스트리 설정 (/usr/local/confluent/etc/schema-registry/schema-registry.properties)
- API 명령어를 실행하여 호환성 레벨을 확인할 수 있다.
  ```
  $ sudo systemctl restart schema-registry
  $ curl -X GET http://peter-kafka03.foo.bar:8081/config
  ```

<br/>

```
{ "namespace": "stduent.avro",
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
```
🔼 이름과 반 필드를 정의한 BACKWARD 호환성 V1 스키마
- 실습용 토픽 구성이 필요하므로, 아래와 같은 속성의 토픽을 생성한다.
  - 이름: peter-avro3
  - 파티션: 1
  - 리플리케이션 팩터: 3
 
- 토픽 생성 후, 정의한 스키마를 이용해 파이썬 에이브로 프로듀서 작성 후 실행한다.

<br/>

```
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro", # BACKWARD 호환성 실습 V1 스키마를 적용한 에이브로 프로듀서
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
"""

# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)
value = {"name": "Peter", "class": 1} # 전송할 메시지

# 전송 결과 확인
def delivery_report(err, msg):
  """ Called once for each message produced to indicate delivery result.
      Triggered by poll() or flush(). """
  if err is not None:
    print('Message delivery failed: {}'.format(err))
  else:
    print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition())

# AvroProducer 속성 정의
avroProducer = AvroProducer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'on_delivery': delivery_report,
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'
  }, default_value_schema=value_schema)

# 메시지 전송
avroProducer.produce(topic='peter-avro3', value=value) # peter-avro3 토픽으로 메시지 발송
avroProducer.flush()
```
🔼 BACKWARD 호환성 V1 스키마를 적용한 에이브로 프로듀서(python-avro-producer_v1.py)

<br/>

```
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer
from confluent_kafka.avro.serializer import SerializerError

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro", # BACKWARD 호환성 실습 V1 스키마를 적용한 에이브로 컨슈머
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
"""

# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)

# AvroConsumer 속성 정의
c = AvroConsumer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'group.id': 'python-groupid01', # 컨슈머 그룹 아이디
  'auto.offset.reset': 'earliest',
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'}, reader_value_schema=value_schema)

# 토픽 구독
c.subscribe(['peter-avro3']) # peter-avro3 토픽을 컨슘

# 메시지 컨슘
while True:
  try:
    msg = c.poll(10)

  except SerializerError as e:
    print("Message deserialization failed for {}: {}".format(msg, e))
    break

  if msg is None:
    continue

  if msg.error():
    print("AvroConsumer error: {}".format(msg.error()))
    continue
    print(msg.value())

# 종료
c.close()
```
🔼 BACKWARD 호환성 V1 스키마를 적용한 에이브로 컨슈머(python-avro-consumer_v1.py)
- 컨슈머를 실행해보면 프로듀서에서 전송한 메시지를 읽어오는 것을 확인할 수 있다.

<br/>

```
{ "namespace": "stduent.avro",
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
    {"name": "age", "type": "int", "default": 1, "doc": "Age of the student"}
  ]
}
```
🔼 이름과 반 필드에 나이 필드를 추가한 BACKWARD 호환성 V2 스키마
- age 필드가 추가됐다.(기본값은 1)
- 해당 V2 스키마를 기반으로 프로듀서와 컨슈머 클라이언트들도 상위 버전의 스키마가 적용되어야 한다.

<br/>

```
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro", # BACKWARD 호환성 실습 V1 스키마를 적용한 에이브로 프로듀서
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"},
    {"name": "age", "type": "int", "default": 1, "doc": "Age of the student"} # 나이 필드를 추가한 BACKWARD 호환성 V2 스키마 적용
  ]
}
"""

# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)
value = {"name": "Peter", "class": 1, "age": 2} # 전송할 메시지

# 전송 결과 확인
def delivery_report(err, msg):
  """ Called once for each message produced to indicate delivery result.
      Triggered by poll() or flush(). """
  if err is not None:
    print('Message delivery failed: {}'.format(err))
  else:
    print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition())

# AvroProducer 속성 정의
avroProducer = AvroProducer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'on_delivery': delivery_report,
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'
  }, default_value_schema=value_schema)

# 메시지 전송
avroProducer.produce(topic='peter-avro3', value=value) # peter-avro3 토픽으로 메시지 발송
avroProducer.flush()
```
🔼 BACKWARD 호환성 V2 스키마를 적용한 에이브로 프로듀서(python-avro-producer_v2.py)
- 프로듀서를 가상환경에서 실행하면 해당 메시지가 전송된다.
- 메시지 전송 후 컨슈머의 메시지를 확인해보면 V2 스키마가 적용되지 않았다는 것을 확인할 수 있다.(컨슈머는 아직 V1이기 때문)

<br/>

```
# http://github.com/confluentinc/confluent-kafka-pyton
# avro 관련 모듈 임포트
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer
from confluent_kafka.avro.serializer import SerializerError

# 스키마 정의
value_schema_str = """
{ "namespace": "stduent.avro", # BACKWARD 호환성 실습 V1 스키마를 적용한 에이브로 컨슈머
  "type": "record",
  "doc": "This is an example of Avro.",
  "name": "Student",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "Name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"},
    {"name": "age", "type": "int", "default": 1, "doc": "Age of the student"} # 나이 필드를 추가한 BACKWARD 호환성 V2 스키마 적용
  ]
}
"""

# 밸류 스키마 로드
value_schema = avro.loads(value_schema_str)

# AvroConsumer 속성 정의
c = AvroConsumer({
  'bootstrap.servers': 'peter-kafka01.foo.bar,peter-kafka02.foo.bar,peter-kafka03.foo.bar',
  'group.id': 'python-groupid01', # 컨슈머 그룹 아이디
  'auto.offset.reset': 'earliest',
  'schema.registry.url': 'http://peter-kafka03.foo.bar:8081'}, reader_value_schema=value_schema)

# 토픽 구독
c.subscribe(['peter-avro3']) # peter-avro3 토픽을 컨슘

# 메시지 컨슘
while True:
  try:
    msg = c.poll(10)

  except SerializerError as e:
    print("Message deserialization failed for {}: {}".format(msg, e))
    break

  if msg is None:
    continue

  if msg.error():
    print("AvroConsumer error: {}".format(msg.error()))
    continue
    print(msg.value())

# 종료
c.close()
```
🔼 BACKWARD 호환성 V2 스키마를 적용한 에이브로 컨슈머(python-avro-consumer_v2.py)
- 프로듀서(V2)를 실행하여 메시지를 다시 전송하고, 컨슈머를 실행시켜보면 age 필드도 추가된 메시지를 읽어옴을 확인할 수 있다.
- 결국 완벽한 스키마 호환성을 유지하기 위해서는 호환성 레벨에서 가이드하는 순서를 잘 따라야 한다. (BACKWARD에서는 consumer -> producer)

<br/>

## 10.5 정리
- 한번 스키마를 깔끔하게 정의해놓는다면, 이후 스키마 변경에 유연하게 대처할 수 있다.
- 스키마 레지스트리를 활용함으로써 데이터 관리적인 측면을 강화한다면 지금보다 더욱 효율적인 데이터 처리와 운영이 가능할 것이다.
