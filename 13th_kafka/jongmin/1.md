# 1장 카프카란 무엇인가
- 링크드인이 대용량, 대규모 메시지를 빠르게 처리하도록 개발하 메시징 플랫폼
- 초기에는 링크드인은 웹사이트에서 발생하는 실시간 로그를 수집하고 처리하기 위해 만듬
- 지금은 이벤트소싱, 비동기 처리 등 다양한 목적을 위해 사용

## 1.1 카프카의 탄생 배경
#### 카프카 적용 전
<img width="1030" alt="image" src="https://user-images.githubusercontent.com/46064193/224053040-b7472fca-2b89-445a-86b0-721af99607c5.png">

#### 데이터 시스템
1. meters(사용량, 응답시간, 에러카운트)를 시계열로 저장해서 처리할 시스템
2. 로그 모니터링(실시간/배치) 시스템
    - 실시간: druid, es(카프카 커넥트로..!)
    - 배치: hdfs, hive
3. 컨텐츠, 고객 정보 제하는 시스템의 OLTP 쿼리
4. 실시간 처리를 위한 키/밸류 저장소
5. 데이터 마켓, 데이터 웨어하우스
6. 하둡

#### 가상 시나리오(앱/서비스 개발팀)
- 사용자 요청 데이터를 디비로, 메트릭/로그를 데이터 스토어로 전달
- 다른 서비스로 비동기로 연동해줘야해서 MQ 사용. 여기선 kafka이전의 기본적인 MQ라서 데이터를 축약해서 넣음
- 키밸류 저장소에도 사용자의 요청을 객체로 만들어 저장
- BI를 위해 사용자 요청데이터를 또 csv포맷으로 바꿔서 저장
...
이런경우 다 하나씩 데이터 연동을 위해 유지 관리를 해줘야함

이렇듯 e2e 환경에서의 복잡도가 증가하고, 데이터 파이프라인의 파편화를 해결하기 위해 카프카 사용.
ex) sdk에서 사용자 이벤트를 발화해서 로그로 카프카에 부어주면, 이걸갖고 여기저기서 꽃아서 카프카 커넥트로 hdfs로 덤프를 떠놔서 활용할 수 있고, 실시간으로 es에 꽃아볼수도있고, druid라는 실시간 olap에도 꽃아볼수있고, 어플리케이션에서 실시간으로 컨슈밍해서 뭔가 로직을 처리할 수도 있고 등등 통합된 하나의 파이프라인

#### 카프카 적용 후
![image](https://user-images.githubusercontent.com/46064193/224061153-a71734ad-6bbc-4ef9-afa7-0c7bdfed5622.png)

- 모든 데이터 스토어와 데이터/이벤트가 카프카를 중심으로 연결
- 다양한 분석, 신뢰성 높은 분석부터 실시간 분석까지
- 매 컨슈머 포맷 생각안해도 카프카에만 갖다 부어주면 나머지는 알아서 가져가도록

#### 카프카의 지향점
- 카프카를 메시지 전달의 중앙 플랫폼으로 두고 기업에서 필요한 모든 데이터 시스템, 마이크로 서비스, saas 서비스와 연결된 파이프라인을 만드는 것

#### 카프카의 현재
- 오픈소스로 공개됨
- confluent라는 회사를 창립해서 계속 발전중
- 오픈소스로 쓰면되지 confluent는 뭐로 돈버냐?
    - 회사에서 카프카 장애가 아주 가끔씩 나는데 메시지 유실도 됨...카프카를 쓰는데 신뢰도 보장하기가 어렵다? 그럼 안되지
    - confluent는? 99.99% 보장해줌
    - 근데 매우 비싸다고함
      <img width="1227" alt="image" src="https://user-images.githubusercontent.com/46064193/224063337-e49152f0-8050-4b76-9210-7beac2047e32.png">

## 1.2 카프카의 동작 방식과 원리
#### 메시징 시스템
- '메시지'라는 데이터 단위를 보내는 '퍼블리셔' || '프로듀서'에서 카프카 '토픽'이라는 메시지 저장소에 데이터를 저장하면, 가져가는 측 '서브스크라이버' || '컨슈머' 가 토픽에서 데이터를 가져감 
- 데이터를 보내고 받는 pub-sub 모델
- 일반적인 통신(ex. tcp)는 endpoint에서 문제가 발생하면 장애가 발생할 수 있고, 매번 endpoint마다 연결해줘야해서 확장성도 떨어짐
- 펍/섭 방식은 컨슈머에게 직접 보내주는게아니라 중간의 메시징 시스템에 전달
- 교환기는 각 컨슈머의 큐에 넣어주고, 컨슈머는 큐를 모니터링하다가 가져감(poll 메소드)
- 그래서 컨슈머가 죽었다 살아나도 다시 메시지를 가져갈 수 있음
- 근데 이거 때문에 메시지가 정확하게 전달되었는지 확인하려면 코드가 복잡해지고..? 
    - 이거 이상한게, acks=all로 프로듀싱해주고, auto-commit=false로 컨슈밍하면 정확하게 전달됨을 보장하고 코드는 딱히 안복잡해지는데..?
    - at-least-once는 그냥 하면되고 exactly-once를 하려고하면 트랜잭션 쓰고 조금씩 복잡해지는..?
    - 아하! 그냥 메시징시스템 얘기구나
- 카프카는 신뢰성 관리를 프로듀서와 컨슈머 쪽으로 넘기고,  교환기 기능도 컨슈머가 만들수 있게 해서 고성능


## 1.3 카프카의 특징
#### 프로듀서와 컨슈머의 분리
- 메시지를 보내는 역할과 받는 역할이 완벽하게 분리된 펍/섭 방식 적용
    - 만약 메트릭 수집할때 카프카를 안쓴다면?
    - 매 서버에 에이전트를 설치하고, 메트릭 수집 서버도 만들어야함
    - 근데 프로메테우스도 이 구조아닌감?

#### 멀티 프로듀서 멀티 컨슈머
- 하나의 토픽에 여러 프로듀서와 여러 컨슈머가 접근 가능함
- 토픽도 여러 토픽에 보낼 수 있음

#### 디스크에 메시지 저장
- 메시지를 디스크에 저장함. 다른 메시징 시스템은 읽으면 바로 큐에서 삭제한다하네요
- 컨슈머가 '메시지 손실 없이' 메시지를 가져갈 수 있다
    - '메시지 손실 없이' 라고 했지 '중복이 없다'고 하진 않았습니다
    - 어디까지 읽었는지 정확한 오프셋 모르면 중복된걸 다시 읽을수밖에
- 리텐션기간 줘서 최근 3개월 이런식으로 유지함

#### 확장성
- 브로커를 여러대로 온라인 상태에서 확장할 수 있음
    - 우린 몰라도댐~

#### 높은 성능
- 고성능을 유지하기 위해 카프카는 내부적으로 분산처리 배치처리 등 다양한 기법을 사용

### 카프카 용어
브로커: 카프카가 설치된 서버
토픽: 프로듀서와 컨슈머들이 카프카로 보낸 메시지(종류)를 구분하기 위한 네임으로 사용
파티션: 병렬처리가 가능하도록 토픽을 나눠 놓는 것. 많은 양의 메시지 처리를 위해 파티션의 수를 '늘려줄 수 있다'
 - 고 했지, 줄일 수 있다고는 안했다. 세그먼트 재배치 비용이 매우 비싸서 제공하지 않음

## 1.4 카프카의 확장과 발전
SOA로 업무를 서비스 단위로 쪼개고 각 서비스간의 연결은 ESB를 통해 연결한다

#### ESB 특징
- 멀티 프로토콜과 데이터 타입 지원
- 느슨한 결합을 위한 메시지 큐 지원
- 이벤트 기반 통신 지향

#### 넷플릭스
![image](https://user-images.githubusercontent.com/46064193/224075543-42adb446-6f03-42ec-a76c-53ade6fc1f68.png)

- 초기에는 배치처리를 위해 하둡에 이벤트를 저장하는것만이 목적
- 실시간 분석 수요가 늘어나면서 카프카 사용
- S3로 넘겨져서(HDFS) + EMR(맵리듀스) = 하둡
- 엘라스틱서치에서 실시간 검색
- 또다른 카프카에 연결
- 블록체인은 패스
