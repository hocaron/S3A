Kafka의 탄생 배경과 특징들에 대해 알아보겠다. 또, 기업들에선 어떻게 Kafka를 도입했는지 알아보겠다.

# Kafka 의 탄생 배경
카프카는 다들 잘 알고 있는 미국의 소셜 네트워크 서비스인 링크드인 에서 처음 출발한 기술로서, 링크드인 사이트가 급속도로 성장할 때 발생하는 내부 이슈들을 해결하기 위해 탄생되었다. 

링크드인에서 Kafka를 도입하기 전에는 아래의 아키텍처를 가지고 서비스되었다.
![](https://velog.velcdn.com/images/ddangle/post/e85e4166-ad0e-4bfe-b31c-a39b99fe893f/image.png)


위의 그림을 보면 카프카를 사용하기 전에는, 각각의 서비스들이 높은 결합도를 가지며 내부적으로 통신하고 있다. 처음에는 고객DB, 스플렁크 정도로 시작하다가 서비스가 커지면서 자연스럽게 데이터 스토어가 분화된다. 

많은 사용자의 요청에도 빠른 응답을 하기 위해 미들웨어로 키/값 스토리지(ex. Redis)에도 사용자 요청 데이터를 넣게 된다고 가정해본다. 서버에서는 사용자 요청 데이터를 특정 객체로 만들고 직렬화(Serialization)해서 키/값 스토리지에 저장한다. 
그리고 비즈니스 분석을 하는 데 고객 요청 데이터가 필요하고, 기존에 데이터베이스, 메시지 큐, 키/값에 넣었던 것 중 하나의 포맷을 사용하려고 한다면, **대부분의 데이터 웨어하우스는 다양한 포맷의 데이터를 지원하지 못하므로** 데이터베이스 컬럼에 맞춰서 CSV 포맷으로 변경해야 한다. 즉, 데이터를 주기적으로 CSV 포맷으로 변형시켜(Transform), 데이터 웨어하우스에 넣는 작업을 하는 앱을 또 만들고 운영해야 한다. 
- 데이터 웨어하우스에서 여러 포맷의 데이터를 지원하지만, 데이터 통합과 일관성을 유지하는 것이 중요하므로, 모든 데이터 형식을 지원하는 것이 어렵다는 의미이다.
- 대부분의 데이터 웨어하우스는 ETL(Extract, Transform, Load) 과정을 통해 데이터가 저장된다. 즉, 데이터를 변형시켜 일관된 포맷으로 저장한다.

> #### 데이터 웨어하우스란?
데이터 웨어하우스는 보다 정보에 입각한 의사 결정을 내릴 수 있도록 분석 가능한 정보의 중앙 리포지토리이다. 데이터 웨어하우스는 다양한 소스(트랜잭션 시스템, 관계형 데이터베이스 등등) 에서 수집된 데이터를 통합하여 일관성 있는 형식으로 저장하며, 이를 기반으로 다양한 분석 작업을 수행할 수 있다. 또한 데이터 웨어하우스는 대량의 데이터를 처리하기 위한 최적화된 구조를 가지고 있으며, 데이터 검색 및 분석 작업에 최적화되어 있다. 
![](https://velog.velcdn.com/images/ddangle/post/1000495b-fd51-4537-b7f8-86d09f4025c4/image.png)

다시 본론으로 돌아가서 정리해보자면, 여러 개의 서비스들이 각각의 방식으로 메시지를 주고받으며 통신하고 있다. 즉, 여러 개의 서비스들이 **높은 결합도** 를 가지고 운영되고 있다.

이런 방식을 `엔드투엔드(end-to-end) 연결 방식의 아키텍처`라고 하는데, 이 방식에는 문제점들이 존재한다.

> #### 엔드투엔드(end-to-end) 연결 방식의 아키텍처
End-to-end (E2E) 연결 방식은 네트워크에서 데이터를 전송하는 방식 중 하나이다. 이 방식은 데이터가 출발지에서 목적지까지 직접적으로 이동하는 것을 의미한다.
이 방식에서는 데이터를 전달하는 모든 단계에서 중간 노드들이 개입되지 않고, 데이터의 처리와 전송은 출발지와 목적지에서만 이루어진다. 즉, 데이터의 전송 경로상에 있는 모든 노드가 단순히 데이터를 전달하고, 데이터를 처리하지 않는다.

### 통합된 전송 영역이 없어서 복잡도 증가
실시간 트랜잭션(OLTP) 처리와 비동기 처리가 동시에 이뤄지지만 통합된 전송 영역이 없어서 복잡도가 증가한다. 즉 각각의 통신 방법으로 여러 서비스들이 통신하기에 관리하는 데에도 많은 문제가 생기고, 이 모든 통신 방법들을 유지보수하기에 굉장히 힘들어진다.

따라서 서버 증설, 하드웨어 증설, 운영체제 업그레이드 등의 작업에서도 많은 준비 시간이 필요해진다. 


### 데이터 파이프라인 관리의 어려움
실시간 트랜잭션 데이터베이스, 아파치 하둡, 모니터링 시스템, 키-값 저장소 등 많은 데이터 시스템들이 있는데, 이 시스템에 저장된 동일한 데이터를 개발 부서들이 각기 다른 방법으로 파이프라인을 만들고 유지하게 된다. 처음에는 간편하더라도, 나중에는 통합 데이터 분석을 위해 서로 연결되어야 하는 일들이 필연적으로 발생한다. 

여러 개의 데이터 파이프라인은 각각 데이터 포맷과 처리하는 방법들이 달라서 데이터 파이프라인을 확장하기 어려웠고, 이러한 데이터 파이프라인들을 조정하고 운영하는 것은 엄청난 노력이 필요해진다.

-----

이런 문제들을 해결하기 위해 **Kafka** 가 만들어졌다. <span style="color:orange">**Kafka의 주요 목적**</span>은 다음과 같았다.
- 프로듀서와 컨슈머 분리
- 메시징 시스템과 같이 영구 메시지 데이터를 여러 컨슈머에게 허용
- 높은 처리량을 위한 메시지 최적화
- 데이터가 증가함에 따라, 스케일 아웃이 가능한 시스템

## 카프카를 적용한 링크드인 서비스 아키텍처
링크드인에서 카프카를 도입하고 나서의 아키텍처는 다음과 같다.

![](https://velog.velcdn.com/images/ddangle/post/96ae27de-74f9-4c20-9c3a-6d2d96635875/image.png)

사내에서 발생하는 모든 이벤트/데이터의 흐름을 중앙에서 관리하는 카프카를 적용한 결과는 굉장히 직관적으로 바뀌었다! 

카프카를 적용했을 때의 이점에 대해 정리해보자.
- 카프카가 전사 데이터 파이프라인으로 동작하기 때문에 모든 데이터 스토어와 서비스에서 발생하는 데이터/이벤트가 카프카를 중심으로 연결되어 있다.
- 새로운 데이터 스토어가 들어와도 서로 카프카가 제공하는 표준 포맷으로 연결되어 있어서 데이터를 주고받는데 부담이 없다.
- 이전에는 할 수 없었던 다양한 분석이 가능해진다.


# 카프카의 동작 원리
메시지를 보내는 측을 `퍼블리셔(Publisher)` 혹은 `프로듀서(Producer)` 라고 하고, 메시지를 가져가는 쪽을 `서브스크라이버(Subscriber)` 혹은 `컨슈머(Consumer)` 라고 한다. 

프로듀서가 메시지를 보내면 카프카에서는 토픽이라는 메시지 저장소를 이용해 메시지를 저장하고, 컨슈머에서는 원하는 토픽에서 메시지를 가져간다. 

중앙에 메시징 시스템 서버를 두고, 이렇게 메시지를 보내고 받는 형태의 통신을 `펍/섭 모델(Pub/Sub model)` 이라고 한다.

> ### Pub/Sub 모델이란?
일반적 형태의 통신(`엔드투엔드 연결방식`)은 통신에 참여하는 개체가 많아질수록 서로 일일이 연결을 하고 데이터를 전송하기에 확장성이 좋지 않다.
하지만, `Pub/Sub 모델`은 프로듀서가 메시지를 중간의 메시징 시스템에 전달함으로써 확장성이 용이해지는 장점을 가진다. 
하지만 `Pub/Sub 모델`은 네트워크를 타고 메시징 시스템에 메시지가 전송되기에 **기존의 일반적인 통신방법(엔드투엔드)에 비해 많이 느려지고, 보내는 데이터의 양을 축소시켜야 했다.** <span style="color:orange">**즉, `Pub/Sub모델` 에는 속도와 용량 측면에서의 단점이 존재한다**.</span> 그래서 `Pub/Sub 모델`은 대규모 데이터를 메시징 시스템을 통해 전달하기 보다는 간단한 이벤트를 서로 전송하는데 주로 사용된다.
![](https://velog.velcdn.com/images/ddangle/post/46430a7c-c70c-4929-b65e-a06d434c2d48/image.png)


카프카는 `기존 Pub/Sub 모델`의 **"속도와 용량의 문제"** 를 해결하기 위해, 카프카는 프로듀서와 컨슈머가 직접 메시지 교환 전달의 신뢰성 관리를 하게 하고, 부하가 많이 걸리는 교환기 기능도 직접하지 않아 메시징 전달에만 집중할 수 있어 성능 향상을 꾀하였다. 또한 카프카 역시 비동기 시스템으로 컨슈머는 프로슈머와 관계없이 데이터를 가져가고, 프로듀서 또한 컨슈머와 관계없이 데이터를 생산한다.

# 카프카의 특징
**기존 메시징 시스템과 차별화되는 특징들** 에 대해 알아본다. 
### 프로듀서와 컨슈머의 분리
링크드인에서 메트릭 수집 방식을 폴링방식으로 구현된 시스템을 사용하면 메트릭 수집이나 처리 시간이 늦어지는 문제점이 있었다. 
> #### 폴링 방식?
폴링(Polling) 방식은 시스템에서 정기적으로 특정한 주기로 데이터를 수집하는 방법입니다. 예를 들어, 메트릭 수집 시스템에서는 정해진 시간 간격으로 서버나 어플리케이션의 상태를 확인하고 데이터를 수집합니다. 
아래의 코드 예시와 같다.
```java
while(true) {
    if (요청 없을 시) {
    	sleep(1000);	
    	continue;
    }
    // 로직
    // 응답
}
```



그래서 카프카는 메시징 전송 방식 중 <span style="color:orange">**메시지를 보내는 역할이 완벽하게 분리된 펍/섭 방식**</span>을 적용했다. 카프카를 이용하면 구조가 단순해지고, 서비스 서버들은 모니터링이나 분석 시스템 등의 상태와 상관없이 카프카로 메시지를 보내는 역할만 함으로써 각자의 역할이 분리되는 장점을 가진다. 또한 서버 추가에 대한 부담도 줄일 수 있다.


### 멀티 프로듀서, 멀티 컨슈머
카프카는 하나의 토픽에 여러 프로듀서 또는 컨슈머들이 접근 가능한 구조로 되었다. 하나의 프로듀서가 하나 이상의 토픽으로 메세지를 보낼 수 있고, 하나의 컨슈머가 하나 이상의 토픽으로부터 메시지를 가져올 수 있다.

이렇기에 카프카는 중앙집중형 구조로 구성할 수 있다.

### 디스크에 메시지 저장
카프카가 기존의 메시징 시스템과 가장 다른 특징 중 하나는 바로 디스크에 메시지를 저장하고 유지하는 것이다. 
- 일반적인 메시징 시스템들은 컨슈머가 메시지를 읽으면 큐에서 메시지를 삭제한다. 
- 하지만, 카프카는 컨슈머가 메시지를 읽어가더라도 정해져 있는 보관주기동안 디스크에 메시지를 저장해둔다. 

메시지가 디스크에 저장되어 있기 때문에, 메시지 손실 없이 작업이 가능하다. 

### 확장성 용이
하나의 카프카 클러스터는 3대의 브로커로 시작해 수십대의 브로커로 확장 가능하다. 확장 작업은 카프카 서비스의 중단 없이 온라인 상태에서 작업이 가능하다. 

### 높은 성능
카프카는 고성능을 유지하기 위해 카프카는 내부적으로 분산 처리, 배치 처리 등 다양한 기법을 사용하고 있다.
- ex) 파티셔너에서는 프로듀서에서 온 메시지들을 모을 수 있는 만큼 모아서, 배치 처리를 해줌으로써 성능을 높였다.

# 카프카의 확장과 발전
카프카가 등장하면서 서비스 기반 아키텍처(SOA, Service Oriented Architecture)의 핵심 구성요소 중 하나인 엔터프라이즈 서비스 버스(ESB, Enterprise Service Bus)를 쉽게 구현할 수 있게 되었다. 

또한, 카프카 스트림즈를 통해 스트림 처리 앱을 개발할 수 있게 되고, KSQL을 통해 SQL 기반으로 데이터를 실시간으로 분석할 수 있게 되었다.
> ### 스트림이란?
스트림(stream)은 시간이 지남에 따라 사용할 수 있게 되는 일련의 데이터 요소를 가리키는 수많은 방식에서 쓰인다. 예시로는, 영상, 음성과 같이 작은 조각들이 하나의 줄기를 이루며 전송되는 데이터 열을 의미한다. 
일반적으로 스트림 데이터는 배치 처리(batch processing)와 달리 실시간으로 처리되며, 데이터가 생성되는 즉시 처리됩니다. 이는 대규모 데이터의 처리와 분석, 실시간 모니터링 등 다양한 분야에서 중요한 역할을 한다.
카프카 스트림즈에서 스트림은 카프카 토픽(topic)에서 생성된 데이터의 연속적인 흐름을 나타낸다. 이러한 스트림 데이터를 처리하고 변환하여 새로운 스트림 데이터를 생성하거나 외부 시스템으로 전달할 수 있다. 이를 통해 실시간으로 데이터를 처리하고 원하는 결과를 도출할 수 있다.

----
# Kafka 기본 개념 및 용어
## 토픽(Topic)
Kafka 에서는 데이터가 들어가는 공간을 토픽이라고 한다. 또한, 하나의 Kafka에서는 여러 개의 토픽을 생성할 수 있다. 프로듀서는 카프카에 데이터를 넣고, 컨슈머는 데이터를 가져가는 구조이다.

또한, 하나의 토픽은 여러 개의 파티션(partition)으로 구성될 수 있다. (파티션 번호는 0번부터 시작한다) 프로듀서가 데이터를 넣고 컨슈머는 파티션에 있는 데이터를 읽는다. 하지만, **카프카에 있는 데이터를 읽는다고 기존 데이터가 삭제되지 않는다는 특징** 을 가진다.
즉, Consumer가 record들을 가져가도 데이터는 삭제되지 않는다! 그렇기에 새로운 Consumer가 해당 파티션에 있는 Record를 가져가려 하면, 0번부터 가져갈 수 있는 것이다.
- 하지만, 새로운 컨슈머가 0번부터 메시지를 가져가려면, 컨슈머 그룹이 달라야 하고, `auto.offset.reset=earliest`여야 한다는 전제조건이 붙는다.

![](https://velog.velcdn.com/images/ddangle/post/d2e0b8dd-7f53-4f00-944c-41e9228fb1a3/image.png)

만약 파티션이 여러 개라면, 프로듀서가 토픽에 데이터를 저장할려고 할 때 데이터는 어느 파티션에 저장될까?
- `key` 값을 `null` 로 지정하지 않고 기본 파티셔너를 사용할 경우 토픽에 메시지를 저장할 때, `라운드 로빈(Round robin)` 방식으로 파티션에 메시지가 저장된다.
- `key` 값을 명시하고, 기본 파티셔너를 사용하면 키의 해시 값(hash) 을 구하고, 특정 파티션에 데이터를 할당한다. 


> 파티션은 늘리는 것은 가능하지만, 파티션의 개수를 줄이는 것을 불가능하니 주의해야 한다.
### 왜 파티션 개수를 늘릴까?
파티션을 늘리면, 컨슈머의 개수를 늘려서 데이터 처리를 분산시킬 수 있기 때문이다.


### Partition의 Record는 언제 삭제되는가?
- `log.retetion.ms` : 최대 record 보존 시간
- `log.retention.byte` : 최대 record 보존 크기(byte)


## 브로커(Broker)  
카프카 브로커(Broker)는 카프카가 설치되어 있는 서버 단위를 말한다. 보통 3개 이상의 브로커를 구성하여 카프카를 운영하는 것을 권장한다.
![](https://velog.velcdn.com/images/ddangle/post/a65f0406-0075-489e-bcdc-9b9215f5a57a/image.png)


## 복제(Replication), ISR(In-Sync-Replication)
**카프카의 가용성** 을 보장하기 위해, `복제(Replication)`을 많이 사용한다. 만약 원본에서 문제가 생기면, 복제본이 원본 역할을 승계하게 됨으로써 **SPOF(Single-Point-Of-Failure) 장애에 대응 가능**해진다. 

- `replication : 1` = 원본 1개만 존재
- `replication : 2` = 원본 1개와 복제본 1개 존재
- `replication : 3` = 원본 1개와 복제본 2개 존재

> 브로커의 개수에 따라, replication의 개수는 제한된다. 브로커의 개수가 3이면, replication은 4 이상이 될 수 없다.

여기서 원본 1개의 partition은 `Leader Partition` 이 되고, 복제본의 partition은 `Follower Partition` 이 된다.
![](https://velog.velcdn.com/images/ddangle/post/1f2863c0-cebf-433c-963b-860bc4d1fc34/image.png)

#### Leader Partition의 역할
프로듀서는 토픽의 파티션에 데이터를 전달할 때, 전달받는 주체가 Leader Partition이 된다. 


#### ack 옵션
프로듀서에는 ack라는 옵션이 있는데, ack 옵션 값에는 0, 1, all 3개 중 하나를 골라 사용한다.
- `ack : 0` = Leader Partition에 데이터 전송 후 응답 값 받지 않는다.
  - **속도는 빠르지만 데이터 유실 가능성 있다.**
  - Leader Partition에 데이터가 정상적으로 전송됐는지 확인할 수 없고, 나머지 Partition에 복제가 잘 이뤄졌는지도 확인 불가능하다. (UDP 프로토콜과 유사)
- `ack : 1` = Leader Partition에 데이터 전송 후 응답 값 받는다.
  - Leader Partition이 데이터를 받은 즉시 브로커에서 장애가 난다면, 나머지 Partition에서 복제 됐는지 알 수 없어서 이 방법 또한 데이터 유실 가능성 있다. 
- `ack : all` = Leader Partition에 데이터 전송 후 응답 값 받고, Follower Partition에서도 응답 값을 받는다.
  - **속도는 느리지만, 데이터 유실 가능성이 없다**

#### Follower Partition의 역할
Leader Partition에서 문제가 생기면, Follower Partition이 Leader Partition의 역할을 승계한다. 

> #### 주의할 점
replication 개수가 너무 많아지면, 그만큼 브로커의 리소스 사용량도 늘어나게 돼서 카프카에 들어오는 데이터량과 저장시간(retention date)를 잘 고려해서 replication 개수를 정해야 한다. 



`Leader Partition`과 `Follower Partition` 들을 합쳐서, `ISR(In-Sync-Replication)` 이라고 부른다.
![](https://velog.velcdn.com/images/ddangle/post/39fc6910-7bdb-42ba-9cd9-135116dc3b06/image.png)



## 파티셔너(Partitioner)
프로듀서가 데이터를 보내면 무조건 `파티셔너(Partitioner)`를 통해서 브로커로 데이터가 전송된다. 파티셔너는 데이터를 어느 파티션에 데이터를 넣을지 결정한다. 
- 레코드의 메시지 키와 값에 따라서 파티션의 위치가 결정된다.

![](https://velog.velcdn.com/images/ddangle/post/9d7632ae-d530-4b33-953a-fecc19894fff/image.png)


프로듀서를 사용할 때, 카프카의 파티셔너 설정을 따로 하지 않는다면, `UniformStickyPartitioner` 로 설정이 된다. 
**이 파티셔너는 메시지 키가 있을 때와 없을 때 다르게 동작**한다. 아래에서 살펴본다. 

#### 메시지 키를 가지는 레코드
메시지 키를 가지는 레코드는 파티셔너에 의해서 특정한 해쉬값이 생성된다. 이 해쉬값을 기준으로 어느 파티션에 들어갈지 결정된다.
- 동일한 메시지 키를 가진 레코드는 동일한 해시값을 생성해내서, 동일한 파티션에 들어가는 것을 보장한다.
- 그렇기에 **순서를 지켜서 처리할 수 있다는 장점** 이 있다.

#### 메시지 키가 없는 레코드
메시지 키가 없는 레코드는 `라운드-로빈 방식`으로 파티션에 레코드가 저장된다.
`UniformStickyPartitioner`는 전통적인 라운드-로빈 방식과는 조금 달리 동작한다. `UniformStickyPartitioner` 는 프로듀서에서 배치로 모을 수 있는 최대한의 레코드들을 모아서 파티션으로 데이터를 보내게 된다.
- 이렇게 배치단위로 데이터를 보낼 때, 라운드로빈 방식으로 돌아가면서 데이터를 넣게 된다.

> 기본 파티셔너와 다른 파티셔너를 원한다면, 카프카에서 제공하는 Partitioner 인터페이스를 구현해 직접 커스터마이징한 파티셔너를 사용할 수도 있다.

## 컨슈머 랙(Consumer Lag)

`컨슈머 랙(Consumer Lag) = 카프카 Lag` 은 카프카를 운영함에 있어서 아주 중요한 모니터링 지표 중 하나이다. 

`컨슈머 랙(Consumer Lag)`은 컨슈머가 마지막으로 읽은 offest과 프로듀서과 마지막으로 넣은 offset의 차이를 말한다.
> 데이터의 offest은 프로듀서가 데이터를 넣을 때 결정된다.

![](https://velog.velcdn.com/images/ddangle/post/0ec35b52-5d90-4d76-8593-3de2fc99d95b/image.png)

**lag의 숫자를 통해 현재 해당 토픽에 대해 파이프라인으로 연계되어 있는 프로듀서와 컨슈머의 상태를 유츄할 수 있다.** 주로 컨슈머의 상태를 볼 때 사용한다.
- lag 값이 크다 : 컨슈머가 읽는 속도가 프로듀서가 넣는 속도에 비해 느리다.
- lag 값이 적다 : 컨슈머가 읽는 속도가 프로듀서가 넣는 속도와 거의 비슷하다.

**토픽에 여러 파티션이 존재하면 여러 Lag이 존재할 수 있다.** 한 개의 토픽과 컨슈머 그룹에 대한 lag이 여러 개 존재할 때, 그 중 가장 높은 숫자의 lag을 `records-lag-max` 라고 부른다.
![](https://velog.velcdn.com/images/ddangle/post/8d1d63ec-6220-4576-ac81-2fda059da722/image.png)


## Burrow - Consumer lag 모니터링 오픈소스
카프카 Lag은 토픽의 가장 최신 오프셋과 컨슈머 오프셋 간의 차이이다. `Kafka-client` 라이브러리를 사용해서 Java 또는 Scala 언어를 통해 `카프카 컨슈머`를 쉽게 구현가능하다. 구현한 카프카 컨슈머를 통해 현재 Lag 정보를 가져올 수 있다.
> 만약, lag을 실시간으로 모니터링하고 싶다면 데이터를 Elasticsearch나 InfluxDB와 같은 저장소에 넣은 뒤, Grafana 대시보드를 통해 확인할 수 있다.

**하지만, 컨슈머 단위에서 lag을 모니터링하고 운영하는 것은 아주 위험하고 운영요소가 많이 들어간다.**

왜냐하면, 컨슈머 로직 단에서 lag을 수집하는 것은 컨슈머 상태에 디펜던시가 걸리기 때문이다. 컨슈머가 비정상적으로 종료되면, 더이상 컨슈머는 lag정보를 보낼 수 없어서 lag을 수집하고 모니터링할 수 없다. 

또한, 컨슈머가 개발될 때마다 해당 컨슈머에 lag 정보를 특정 저장소에 저장할 수 있도록 로직을 개발해야 한다. 만약 컨슈머 lag을 수집할 수 없는 컨슈머라면, lag을 모니터링 할 수 없으므로 운영이 매우 까다로워진다.

그래서 카프카의 컨슈머 lag을 효과적으로 모니터링할 수 있는 오픈소스인 `Burrow`를 사용해야 한다. 
![](https://velog.velcdn.com/images/ddangle/post/dbe11a4c-d3fd-4ca9-be3c-3bb8eff5eed4/image.png)



### Burrow의 주요 3가지 특징
#### <span style="color:orange">멀티 카프카 클러스터 지원</span>
카프카 클러스터가 여러 개 있어도, Burrow 어플리케이션 하나만 연동해도 카프카 클러스터에 붙은 컨슈머의 lag을 모두 모니터링할 수 있다.

#### <span style="color:orange">Sliding window를 통한 Consumer의 status 확인</span>
Burrow에선, Consumer의 status를 `ERROR`, `WARNING`, `OK`로 표현한다.
- 만약 데이터 양이 일시적으로 많아지면, consumer offset이 증가하게 되면 `WARNING`으로 정의된다.
- 만약 데이터가 많아지는데 컨슈머가 데이터를 읽지 않으면 `ERROR`로 정의해서 컨슈머가 문제가 있는지 알 수 있다.
  
#### <span style="color:orange">HTTP API 제공</span>
앞의 정보들을 모두 HTTP API를 통해 조회 가능하다.


# Kafka 와 RabbitMQ, Redis queue와의 차이
위처럼 메시징 플랫폼으로 불리는 것들(Kafka, RabbitMQ, Redis Queue)은 2가지로 나뉜다. 하나는 메시지 브로커, 다른 하나는 이벤트 브로커 이다. RabbitMQ와 Redis Queue는 메시지 브로커에 해당하고, Kafka는 이벤트 브로커에 속한다.

메시지 브로커는 이벤트 브로커로서의 역할을 수행할 수 없지만, 이벤트 브로커는 메시지 브로커의 역할을 수행할 수 있다.

메시지 브로커는 대규모 메시지 기반 미들웨어 아키텍처에서 사용되어 왔다. 
> #### 메시지 기반 아키텍처와 이벤트 기반 아키텍처
**메시지 기반 미들웨어 아키텍처(Message-Oriented Middleware Architecture)** 는 분산 시스템에서 애플리케이션 간 통신을 위해 메시지 큐를 사용하는 아키텍처이다. 이 아키텍처에선 메시지 큐가 미들웨어 역할을 수행하여 애플리케이션 사이의 비동기식 통신을 관리한다. 
**이벤트 기반 미들웨어 아키텍처(Event-Driven Middleware Architecture)** 는 이벤트를 중심으로 애플리케이션 간 통신을 구성하는 아키텍처이다. 이벤트는 시스템 내에서 발생하는 모든 상황을 의미하며, 이를 기반으로 애플리케이션은 이벤트를 감지하고 이에 대한 응답으로 동작한다. 메시지 기반 아키텍처와 유사하지만, 이벤트가 중심이 되므로 이벤트에 대한 응답으로 애플리케이션이 동작하는 것이 특징이다. 이 아키텍처는 실시간성이 중요한 분야에서 많이 사용된다.. 

메시지 브로커에 있는 메시지를 받아서 적절히 처리하고 나면, 즉시 또는 짧은 시간 내에 삭제되는 구조이다. 반면에 이벤트 브로커는 이와는 다르다. 
- 이벤트 또는 메시지라 불리는 것은 `레코드`를 의미한다.

## 이벤트 브로커의 특징
#### 레코드를 보관하고 인덱스르 통해 개별 액세스를 관리
#### 업무상 필요한 시간동안 이벤트 보존 가능
즉, 차이점을 정리하면 메시지 브로커는 메시지를 받고나면 메시지를 삭제하지만, 이벤트 브로커는 삭제하지 않는다.

### 메시지를 삭제하지 않는 이유
이벤트 브로커는 서비스에서 나오는 이벤트를 마치 데이터베이스에 저장하듯이 이벤트 브로커의 큐에 저장한다. 이렇게 했을 때의 이점이 존재한다.
- 한 번 일어난 이벤트 데이터를 브로커에 저장함으로써 단일 진실 공급원으로 사용할 수 있다.
- 장애가 발생했을 때, 장애가 일어난 지점부터 재처리할 수 있다.
- 많은 양의 실시간 스트림 데이터를 효과적으로 처리할 수 있다는 특징을 가진다.